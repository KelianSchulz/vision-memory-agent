# ğŸ§  Vision Memory Agent

A lightweight semantic image selection agent that learns to **remember only the most relevant images** for a given textual goal â€“ and **explains why**.

---

## ğŸ¯ Project Goal

This project aims to build a vision-based learning agent that:
- Receives a **semantic goal** (e.g. "Space Exploration", "Urban Life")
- Reviews a set of images
- Selects only those **visually aligned** with the goal
- Justifies its decisions with **semantic keywords** (e.g. "astronaut", "rocket")

---

## ğŸ§  Core Idea

Powered by [CLIP](https://openai.com/research/clip), the agent maps **text and image** into the same vector space.  
This allows the agent to measure semantic similarity between:
- a **goal string**
- and each **image's content**

Then it:
- selects the most relevant images
- stores them as its **visual memory**
- and explains *why* they were selected

---

## ğŸ§± Planned Architecture

```text
goal: "Space Exploration"
        â†“
image folder â”€â”€â”€â”€â”€â–º embed each image
        â†“
embed goal text
        â†“
cosine similarity (goal â†” image)
        â†“
select top images (above threshold)
        â†“
generate explanation
        â†“
store result in memory.json
```

---

## ğŸ“¦ Project Structure (planned)

```
vision-memory-agent/
â”œâ”€â”€ data/                  # images + memory files
â”œâ”€â”€ code/                   # model logic (embed, select, explain)
â”œâ”€â”€ main.py                # agent entry point
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md              # this file
â””â”€â”€ vision_architecture.md # detailed technical design
```

---

## ğŸ”® Future Directions

- Use image captioning models to improve explanation
- Incorporate attention heatmaps for visual justification
- Extend to multi-modal memory: combine text + image selection
- Eventually: let agent select its **own visual learning goal**

---

## âœï¸ Author

Built by Kelian Schulz â€“ as part of a deeper exploration into  
interpretable, goal-driven agents for applied AI research.
